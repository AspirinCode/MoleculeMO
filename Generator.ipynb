{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#Set device\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    device = torch.device(\"cuda\")\n",
    "    cuda = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    cuda = False\n",
    "    \n",
    "print(\"Device =\", device)\n",
    "gpus = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: torch.Size([25246858, 1, 53])\n"
     ]
    }
   ],
   "source": [
    "#Load SMILES data as one-hot encoding\n",
    "data = np.load(\"ohesmiles.npz\")\n",
    "data = data[\"arr_0\"]\n",
    "\n",
    "data = torch.from_numpy(data).view(np.shape(data)[0], 1, np.shape(data)[1])\n",
    "\n",
    "print(\"Dataset size: \" + str(data.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\\n' 0 1.0 ... 0.0 0.0 0.0]\n",
      " ['#' 1 0.0 ... 0.0 0.0 0.0]\n",
      " ['(' 2 0.0 ... 0.0 0.0 0.0]\n",
      " ...\n",
      " ['r' 50 0.0 ... 1.0 0.0 0.0]\n",
      " ['s' 51 0.0 ... 0.0 1.0 0.0]\n",
      " ['t' 52 0.0 ... 0.0 0.0 1.0]]\n",
      "Vocab encodings size: (53, 55)\n"
     ]
    }
   ],
   "source": [
    "#Load vocab dictionary as numpy object array\n",
    "vocab = np.load(\"vocab.npy\")\n",
    "print(vocab)\n",
    "print(\"Vocab encodings size: \" + str(np.shape(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        #Model parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #Model layers\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    #Define initial hidden and cell states\n",
    "    def init_states(self, num_layers, hidden_size):\n",
    "        hidden = [Variable(torch.zeros(num_layers, 1, hidden_size)),\n",
    "                  Variable(torch.zeros(num_layers, 1, hidden_size))]\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "    #Define forward propagation\n",
    "    def forward(self, inp, hidden):\n",
    "        output, hidden = self.lstm(inp, hidden)\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize model and generation parameters\n",
    "input_size = np.shape(data)[2]\n",
    "hidden_size = 1024\n",
    "num_layers = 3\n",
    "dropout = .2\n",
    "learning_rate = 0.001\n",
    "epochs = 1\n",
    "seq_length = 75\n",
    "batch_size = 64\n",
    "\n",
    "char_to_gen = 1000\n",
    "temperature = .7\n",
    "prime_string = \"G\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call and load model\n",
    "model = Model(input_size, hidden_size, num_layers, dropout)\n",
    "model.load_state_dict(torch.load(\"jan15LSTM1024-1.pth\"))\n",
    "\n",
    "#Run on GPU\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prime_string, char_to_gen, temperature):\n",
    "    #SMILES character string\n",
    "    mol = \"G\"\n",
    "    \n",
    "    #Get input tensor from prime string\n",
    "    prediction = torch.from_numpy(vocab[np.where(vocab == str(prime_string))[0], :][:, 2:].astype(float)).view(1,1,-1).cuda()\n",
    "    \n",
    "    hidden = model.init_states(num_layers, hidden_size)\n",
    "    if cuda:\n",
    "        hidden = (hidden[0].cuda(), hidden[1].cuda())\n",
    "    \n",
    "    for i in range(char_to_gen):\n",
    "        #Get input tensor\n",
    "        inp = prediction[i,:,:].view(1,1,-1).float()\n",
    "            \n",
    "        #Run on GPU if available\n",
    "        if cuda:\n",
    "            inp = inp.cuda()\n",
    "                \n",
    "        #Run model\n",
    "        output, hidden = model(inp, hidden)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        char = torch.from_numpy(vocab[top_i,2:].astype(float)).view(1,1,-1).cuda()\n",
    "        \n",
    "        #Update total prediction with the new character\n",
    "        prediction = torch.cat((prediction, char), 0)\n",
    "        \n",
    "        #Update character string\n",
    "        smile = vocab[top_i,0]\n",
    "        mol = mol + str(smile)\n",
    "        \n",
    "    return prediction, mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCOc1ccccc1N2CCN(CCN3C=CC(=O)CC3)c4cc(Cl)ccc4l\n",
      "GCCOC(=O)c1ncn2CCCN(Cc3ccc(cc3)C(=O)N)cc1O\n",
      "GCCC1(C)CC(=O)N(CCN2CCN(CC2)c3ccccn3)C(=O)C4=C(CCC4)O1\n",
      "GCCc1ccc2nc3c(cccc3cc2c1)C(=O)NCCN(C)C\n",
      "GCCCCN(C)C(=O)[C@H](C1CCCCC1)NC(=O)[C@H](CC(C)C)NC(=O)Cc2cc(OC)cc(OC)c2\n",
      "GCCNCC(O)COc1ccc2C(=O)C=C(Oc2c1)c3ccccc3\n",
      "GOC(=O)CCNC(=O)NNC(=O)CCCNc1ccccn1\n",
      "GFc1ccc(cc1)C2OOC3C4CCC(C4)C3(OO2)c5ccccc5\n",
      "G[I-].C[C@@H]1O[C@H](C[N+](C)(C)C)CS1(=O)=O)C(Cc2ccccc2)NC(=O)NCc3ccccc3\n",
      "GC[C@@H](Nc1nc(N)c2cnn(c3ccccc3)c2n1)C(=O)OCC(C)(C)C\n",
      "GCOc1ccc(cc1OC)N(C)Cc2ccc3nc(N)nc(N)c3n2\n",
      "GCOC1=C(N)C(=O)c2c(ccnc2c3ccccn3)C1=O\n",
      "GCCCCN(C)C(=O)[C@H](C1CCCCC1)NC(=O)[C@H](CC(C)C)NC(=O)Cc2cccc(C)c2\n",
      "GCc1cc2c(OC[C@@H](O)CN3CCC(CC3)c4cc5cc(Cl)ccc5s4)cccc2[nH]1\n",
      "GCCCOc1cccc(c1)N2CCN(CCCN3C(=O)CCc4c(Cl)cccc34)CC2\n",
      "GCCOC(=O)c1ncc(n1)c2ccc(CCc3ccccc3)cc2\n",
      "GCCC(CCCN1CCCC(C1)C2OC3CC3)CC2\n",
      "GOC(=O)c1ccc(cc1)C(=O)Nc2ccc3c(OCc4cccc(Br)cc4)cccc3c2\n",
      "GCOc1cccc2CN(CCCc3c[nH]c4ccc(F)cc34)CCc12\n",
      "GOC(=O)CN1C(=O)N(Cc2ccccc2)C(=O)C1=O\n",
      "GCC(=O)c1ccc(NC(=O)CCN2CCN(CC2)c3ccccn3\n"
     ]
    }
   ],
   "source": [
    "prediction, mol = generate(prime_string, char_to_gen, temperature)\n",
    "print(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
