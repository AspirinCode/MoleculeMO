{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import time\n",
    "\n",
    "#Set device\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    device = torch.device(\"cuda\")\n",
    "    cuda = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    cuda = False\n",
    "    \n",
    "print(\"Device =\", device)\n",
    "gpus = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_elapsed(start_time):\n",
    "    elapsed = time.time() - start_time\n",
    "    hours = int(elapsed/3600)\n",
    "    minutes = int(int(elapsed/60)%60)\n",
    "    seconds = int(elapsed%60)\n",
    "    \n",
    "    return hours, minutes, seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: torch.Size([24825178, 1, 53])\n"
     ]
    }
   ],
   "source": [
    "#Load SMILES data as one-hot encoding\n",
    "data = np.load(\"ohesmiles.npz\")\n",
    "data = data[\"arr_0\"]\n",
    "\n",
    "data = torch.from_numpy(data).view(np.shape(data)[0], 1, np.shape(data)[1])\n",
    "\n",
    "print(\"Dataset size: \" + str(data.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\\n' 0 1.0 ... 0.0 0.0 0.0]\n",
      " ['#' 1 0.0 ... 0.0 0.0 0.0]\n",
      " ['(' 2 0.0 ... 0.0 0.0 0.0]\n",
      " ...\n",
      " ['r' 50 0.0 ... 1.0 0.0 0.0]\n",
      " ['s' 51 0.0 ... 0.0 1.0 0.0]\n",
      " ['t' 52 0.0 ... 0.0 0.0 1.0]]\n",
      "Vocab encodings size: (53, 55)\n"
     ]
    }
   ],
   "source": [
    "#Load vocab dictionary as numpy object array\n",
    "vocab = np.load(\"vocab.npy\")\n",
    "print(vocab)\n",
    "print(\"Vocab encodings size: \" + str(np.shape(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        #Model parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #Model layers\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    #Define initial hidden and cell states\n",
    "    def init_states(self, num_layers, hidden_size):\n",
    "        hidden = [Variable(torch.zeros(num_layers, 1, hidden_size)),\n",
    "                  Variable(torch.zeros(num_layers, 1, hidden_size))]\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "    #Define forward propagation\n",
    "    def forward(self, inp, hidden):\n",
    "        output, hidden = self.lstm(inp, hidden)\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize model and generation parameters\n",
    "input_size = np.shape(data)[2]\n",
    "hidden_size = 1024\n",
    "num_layers = 3\n",
    "dropout = .2\n",
    "learning_rate = 0.001\n",
    "epochs = 1\n",
    "seq_length = 75\n",
    "batch_size = 128\n",
    "temperature = .75\n",
    "char_to_gen = 10000\n",
    "runs = 750 #GPU can't handle generating larger amounts of characters at once, so done in a loop\n",
    "prime_string = \"G\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call and load model\n",
    "model = Model(input_size, hidden_size, num_layers, dropout)\n",
    "model.load_state_dict(torch.load(\"network.pth\"))\n",
    "\n",
    "#Run on GPU\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prime_string, char_to_gen, temperature):\n",
    "    \n",
    "    #SMILES character string\n",
    "    mol = \"G\"\n",
    "    \n",
    "    #Get input tensor from prime string\n",
    "    prediction = torch.from_numpy(vocab[np.where(vocab == str(prime_string))[0], :][:, 2:].astype(float)).view(1,1,-1).cuda()\n",
    "    \n",
    "    hidden = model.init_states(num_layers, hidden_size)\n",
    "    if cuda:\n",
    "        hidden = (hidden[0].cuda(), hidden[1].cuda())\n",
    "    \n",
    "    for i in range(char_to_gen):\n",
    "        #Get input tensor\n",
    "        inp = prediction[i,:,:].view(1,1,-1).float()\n",
    "            \n",
    "        #Run on GPU if available\n",
    "        if cuda:\n",
    "            inp = inp.cuda()\n",
    "                \n",
    "        #Run model\n",
    "        output, hidden = model(inp, hidden)\n",
    "        \n",
    "        #Apply softmax to convert output into probabilities\n",
    "        output = F.softmax((output / temperature), dim=2)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1)\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        char = torch.from_numpy(vocab[top_i,2:].astype(float)).view(1,1,-1).cuda()\n",
    "        \n",
    "        #Update total prediction with the new character\n",
    "        prediction = torch.cat((prediction, char), 0)\n",
    "        \n",
    "        #Update character string\n",
    "        smile = vocab[top_i,0]\n",
    "        mol = mol + str(smile)\n",
    "        mol = mol.replace(\"G\", \"\")\n",
    "             \n",
    "    return prediction, mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES run: 0 saved. | Time elapsed: 00h 00 m 01 s\n",
      "SMILES run: 1 saved. | Time elapsed: 00h 00 m 02 s\n",
      "SMILES run: 2 saved. | Time elapsed: 00h 00 m 03 s\n",
      "SMILES run: 3 saved. | Time elapsed: 00h 00 m 04 s\n",
      "SMILES run: 4 saved. | Time elapsed: 00h 00 m 06 s\n",
      "SMILES run: 5 saved. | Time elapsed: 00h 00 m 07 s\n",
      "SMILES run: 6 saved. | Time elapsed: 00h 00 m 08 s\n",
      "SMILES run: 7 saved. | Time elapsed: 00h 00 m 09 s\n",
      "SMILES run: 8 saved. | Time elapsed: 00h 00 m 11 s\n",
      "SMILES run: 9 saved. | Time elapsed: 00h 00 m 12 s\n",
      "SMILES run: 10 saved. | Time elapsed: 00h 00 m 13 s\n",
      "SMILES run: 11 saved. | Time elapsed: 00h 00 m 14 s\n",
      "SMILES run: 12 saved. | Time elapsed: 00h 00 m 16 s\n",
      "SMILES run: 13 saved. | Time elapsed: 00h 00 m 17 s\n",
      "SMILES run: 14 saved. | Time elapsed: 00h 00 m 18 s\n",
      "SMILES run: 15 saved. | Time elapsed: 00h 00 m 19 s\n",
      "SMILES run: 16 saved. | Time elapsed: 00h 00 m 20 s\n",
      "SMILES run: 17 saved. | Time elapsed: 00h 00 m 22 s\n",
      "SMILES run: 18 saved. | Time elapsed: 00h 00 m 23 s\n",
      "SMILES run: 19 saved. | Time elapsed: 00h 00 m 24 s\n",
      "SMILES run: 20 saved. | Time elapsed: 00h 00 m 25 s\n",
      "SMILES run: 21 saved. | Time elapsed: 00h 00 m 27 s\n",
      "SMILES run: 22 saved. | Time elapsed: 00h 00 m 28 s\n",
      "SMILES run: 23 saved. | Time elapsed: 00h 00 m 29 s\n",
      "SMILES run: 24 saved. | Time elapsed: 00h 00 m 30 s\n",
      "SMILES run: 25 saved. | Time elapsed: 00h 00 m 32 s\n",
      "SMILES run: 26 saved. | Time elapsed: 00h 00 m 33 s\n",
      "SMILES run: 27 saved. | Time elapsed: 00h 00 m 34 s\n",
      "SMILES run: 28 saved. | Time elapsed: 00h 00 m 35 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-959e9b0321dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#Generate molecules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprime_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_to_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#Add to file of generated molecules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-0a652b877573>\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(prime_string, char_to_gen, temperature)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#Run model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m#Apply softmax to convert output into probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-f0416f0557dd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inp, hidden)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m#Define forward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 179\u001b[1;33m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for i in range(runs):\n",
    "    \n",
    "    #File to save generated molecules in\n",
    "    new = open(\"generatedsmiles.txt\", \"a\")\n",
    "\n",
    "    #Generate molecules\n",
    "    prediction, mol = generate(prime_string, char_to_gen, temperature)\n",
    "\n",
    "    #Add to file of generated molecules\n",
    "    new.write(mol)\n",
    "\n",
    "    hours, minutes, seconds = time_elapsed(start_time)\n",
    "    print(\"SMILES run: \" + str(i) + \" saved.\" + \" | Time elapsed: {0:02d}\".format(hours) + \"h {0:02d}\".format(minutes) + \" m {0:02d}\".format(seconds) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
