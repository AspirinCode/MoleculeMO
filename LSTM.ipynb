{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x172ff9ab430>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set random seed\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHEMBL canonical SMILES data consists of 51 unique characters: one hot embedding with vocabulary size 51\n",
    "\n",
    "#Input of size [sequence length, batch size, embedding dimension] = [length of SMILES string, batch dize, 51]\n",
    "#Output of size [sequence length, batch size, hidden dimension] = [length of SMILES string, batch size, 1024]\n",
    "lstm = torch.nn.LSTM(input_size = 51, hidden_size = 1024, num_layers = 3, dropout = 0.2)\n",
    "\n",
    "#Transform outputs to have dimensions of input (from hidden dimension to embedding demension)\n",
    "linear = torch.nn.Linear(1024, 51)\n",
    "\n",
    "#Softmax function\n",
    "softmax = torch.nn.functional.softmax\n",
    "\n",
    "#Loss function\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#Adam optimizer \n",
    "optimizer = torch.optim.Adam(list(lstm.parameters()) + list(linear.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "accuracies, max_accuracy = [], 0\n",
    "for x in range(num_epochs):\n",
    "    print('Epoch: {}'.format(x))\n",
    "    for encrypted, original in dataset(num_examples):\n",
    "        # encrypted.size() = [64]\n",
    "        lstm_in = embed(encrypted)\n",
    "        # lstm_in.size() = [64, 51]. This is a 2D tensor, but LSTM expects \n",
    "        # a 3D tensor. So we insert a fake dimension.\n",
    "        lstm_in = lstm_in.unsqueeze(1)\n",
    "        # lstm_in.size() = [64, 1, 51]\n",
    "        # Get outputs from the LSTM.\n",
    "        lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden())\n",
    "        # lstm_out.size() = [64, 1, 1024]\n",
    "        # Apply the affine transform.\n",
    "        scores = linear(lstm_out)\n",
    "        # scores.size() = [64, 1, 1024], but loss_fn expects a tensor\n",
    "        # of size [64, 1024, 1]. So we switch the second and third dimensions.\n",
    "        scores = scores.transpose(1, 2)\n",
    "        # original.size() = [64], but original should also be a 2D tensor\n",
    "        # of size [64, 1]. So we insert a fake dimension.\n",
    "        original = original.unsqueeze(1)\n",
    "        # Calculate loss.\n",
    "        loss = loss_fn(scores, original) \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    print('Loss: {:6.4f}'.format(loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
